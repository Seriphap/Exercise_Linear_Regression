{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPbGX+buAAmiieFSjpbTai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seriphap/Exercise_Linear_Regression/blob/main/HW_1_1_Regression_6710422002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normal Equation (Sklearn)"
      ],
      "metadata": {
        "id": "5TmBLkT3XDLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "x = np.array([[1,0],\n",
        "              [1,2],\n",
        "              [1,3]])\n",
        "\n",
        "y = np.array([1,1,4])\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(x, y)\n",
        "print(f'Intercept: {lin_reg.intercept_:.2f}')\n",
        "print('Coefficient:', [float(f'{coef:.2f}') for coef in lin_reg.coef_])\n",
        "\n",
        "#prediction\n",
        "y_p = lin_reg.predict(x)\n",
        "mse = mean_squared_error(y, y_p)\n",
        "print(f\"MSE = {mse:.2f}\")\n",
        "\n",
        "#y_p = [round(num) for num in y_p] #y_p = [float(f'{num:.2f}') for num in y_p]\n",
        "#print(\"y predict(round) = \", y_p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wz7U0mVBXBEu",
        "outputId": "13aba434-e7e1-4b7e-d328-8e89640acb8b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 0.57\n",
            "Coefficient: [0.0, 0.86]\n",
            "MSE = 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Gradient Descent"
      ],
      "metadata": {
        "id": "mZNGDv9s2lDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "\n",
        "# Cost Function\n",
        "def cost_function(X, y, theta):\n",
        "    N = len(y)\n",
        "    Hypothesis = np.dot(X, theta)\n",
        "    cost = (1/(2*N)) * np.sum(np.square(Hypothesis - y))\n",
        "    return cost\n",
        "\n",
        "# Function to perform gradient descent\n",
        "def gradient_descent(X, y, theta, learning_rate, iterations, ep):\n",
        "    N = len(y)\n",
        "    cost_history = []\n",
        "    #cost_history = np.zeros(iterations)\n",
        "\n",
        "    # Early stopping condition based on cost change\n",
        "    for i in range(iterations):\n",
        "        Hypothesis = np.dot(X, theta)\n",
        "        theta = theta - (learning_rate / N) * np.dot(X.transpose(), (Hypothesis - y))\n",
        "        cost_history.append(cost_function(X, y, theta))\n",
        "        #cost_history[i] = cost_function(X, y, theta)\n",
        "\n",
        "        if i > 0 and np.abs(cost_history[i] - cost_history[i - 1]) <= ep:\n",
        "          return theta, cost_history, i+1\n",
        "          #return theta, cost_history[:i+1], i+1\n",
        "    return theta, cost_history, iterations\n",
        "\n",
        "# Data\n",
        "X = np.array([[1,0],\n",
        "              [1,2],\n",
        "              [1,3]])\n",
        "y = np.array([1,1,4])\n",
        "theta = np.array([0.1,0.1]) #theta for first iteration\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "iterations = 10000\n",
        "limited_iterations = iterations\n",
        "ep = 0.0000000001\n",
        "\n",
        "# Performing gradient descent\n",
        "theta, cost_history, actual_iterations = gradient_descent(X, y, theta, learning_rate, iterations, ep)\n",
        "\n",
        "print(\"Theta:\", theta)\n",
        "print(\"Iterations:\", str(actual_iterations),\"/\",str(limited_iterations))\n",
        "print(\"J:\", cost_history[-1])\n",
        "#print(\"J_history:\", cost_history)\n",
        "\n",
        "print(\"Result: y = \",str(np.round(theta[0],2)),\"+\",str(np.round(theta[1],2)),\"x\")\n"
      ],
      "metadata": {
        "id": "3TJE09HifCdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded0cdfe-a0b1-4e9e-aa5a-2e11145adaa3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theta: [0.57113115 0.85726605]\n",
            "Iterations: 1972 / 10000\n",
            "J: 0.428571444616486\n",
            "Result: y =  0.57 + 0.86 x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "ednaR6SQ21CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "\n",
        "# Cost Function\n",
        "def cost_function(X, y, theta):\n",
        "    N = len(y)\n",
        "    Hypothesis = np.dot(X, theta)\n",
        "    cost = (1/(2*1)) * np.sum(np.square(Hypothesis - y))\n",
        "    return cost\n",
        "\n",
        "# Function to perform stochastic gradient descent\n",
        "def stochastic_gradient_descent(X, y, theta, learning_rate, iterations, ep):\n",
        "    N = len(y)\n",
        "    cost_history = []\n",
        "    #cost_history = np.zeros(iterations)\n",
        "\n",
        "    for i in range(iterations):\n",
        "      for j in range(N):\n",
        "        x_j = X[j]  # Single training example\n",
        "        y_j = y[j]  # Single target value\n",
        "\n",
        "        Hypothesis = np.dot(x_j, theta)\n",
        "        theta = theta - (learning_rate / 1) * np.dot(x_j.transpose(), (Hypothesis - y_j))\n",
        "\n",
        "      #print(theta)\n",
        "      cost_history.append(cost_function(X, y, theta))\n",
        "      #cost_history[i] = cost_function(X, y, theta)\n",
        "\n",
        "      # Early stopping condition based on cost change\n",
        "      if i > 0 and np.abs(cost_history[i] - cost_history[i - 1]) <= ep:\n",
        "        return theta, cost_history, i+1\n",
        "        #return theta, cost_history[:i+1], i+1\n",
        "\n",
        "    return theta, cost_history, iterations\n",
        "\n",
        "# Data\n",
        "X = np.array([[1,0],\n",
        "              [1,2],\n",
        "              [1,3]])\n",
        "y = np.array([1,1,4])\n",
        "theta = np.array([0.1,0.1]) #theta for first iteration\n",
        "\n",
        "#shuffle data\n",
        "Random_row=np.random.permutation(X.shape[0])\n",
        "X=X[Random_row]\n",
        "y=y[Random_row]\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "iterations = 10000\n",
        "limited_iterations = iterations\n",
        "ep = 0.0000000001\n",
        "\n",
        "# Performing stochastic gradient descent\n",
        "theta, cost_history, actual_iterations = stochastic_gradient_descent(X, y, theta, learning_rate, iterations, ep)\n",
        "\n",
        "print(\"Theta:\", theta)\n",
        "print(\"Iterations:\", str(actual_iterations),\"/\",str(limited_iterations))\n",
        "print(\"J:\", cost_history[-1])\n",
        "#print(\"J_history:\", cost_history)\n",
        "\n",
        "print(\"Result: y = \",str(np.round(theta[0],2)),\"+\",str(np.round(theta[1],2)),\"x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUb2YlAL_m8b",
        "outputId": "20ba5130-38c3-4207-a27b-4a538e6c25b1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theta: [0.5622679  0.85114298]\n",
            "Iterations: 1197 / 10000\n",
            "J: 1.2863489672708541\n",
            "Result: y =  0.56 + 0.85 x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini-batch Gradient Descent"
      ],
      "metadata": {
        "id": "BBirFF6autd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "\n",
        "# Cost Function\n",
        "def cost_function(X, y, theta,R):\n",
        "    N = len(y)\n",
        "    Hypothesis = np.dot(X, theta)\n",
        "    cost = (1/(2*R)) * np.sum(np.square(Hypothesis - y))\n",
        "    return cost\n",
        "\n",
        "# Function to perform mini-batch gradient descent\n",
        "def mini_batch_gradient_descent(X, y, theta, learning_rate, iterations, ep, R):\n",
        "    N = len(y)\n",
        "    cost_history = []\n",
        "    #cost_history = np.zeros(iterations)\n",
        "\n",
        "    for i in range(iterations):\n",
        "      for j in range(0,N,R): # Start=0, Stop=N(exclude), Step=R\n",
        "        x_batch = X[j:j+R]  # Select batch of data\n",
        "        y_batch = y[j:j+R]  # Select batch of target value\n",
        "\n",
        "        Hypothesis = np.dot(x_batch, theta)\n",
        "        theta = theta - (learning_rate / R) * np.dot(x_batch.transpose(), (Hypothesis - y_batch))\n",
        "\n",
        "      cost_history.append(cost_function(X, y, theta,R))\n",
        "      #cost_history[i] = cost_function(X, y, theta)\n",
        "\n",
        "      # Early stopping condition based on cost change\n",
        "      if i > 0 and np.abs(cost_history[i] - cost_history[i - 1]) <= ep:\n",
        "        return theta, cost_history, i+1\n",
        "        #return theta, cost_history[:i+1], i+1\n",
        "\n",
        "    return theta, cost_history, iterations\n",
        "\n",
        "# Data\n",
        "X = np.array([[1,0],\n",
        "              [1,2],\n",
        "              [1,3]])\n",
        "y = np.array([1,1,4])\n",
        "theta = np.array([0.1,0.1]) #theta for first iteration\n",
        "\n",
        "#shuffle data\n",
        "Random_row=np.random.permutation(X.shape[0])\n",
        "X=X[Random_row]\n",
        "y=y[Random_row]\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "iterations = 10000\n",
        "limited_iterations = iterations\n",
        "ep = 0.0000000001\n",
        "R = 2 #Mini-batch size (choose R from N)\n",
        "\n",
        "# Performing mini-batch gradient descent\n",
        "theta, cost_history, actual_iterations = mini_batch_gradient_descent(X, y, theta, learning_rate, iterations, ep, R)\n",
        "\n",
        "print(\"Batch size:\", R)\n",
        "print(\"Theta:\", theta)\n",
        "print(\"Iterations:\", str(actual_iterations),\"/\",str(limited_iterations))\n",
        "print(\"J:\", cost_history[-1])\n",
        "#print(\"J_history:\", cost_history)\n",
        "\n",
        "print(\"Result: y = \",str(np.round(theta[0],2)),\"+\",str(np.round(theta[1],2)),\"x\")"
      ],
      "metadata": {
        "id": "RTorSOD8_l80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "077173c9-2465-4ff9-c4c7-c667fa86056a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 2\n",
            "Theta: [0.57143443 0.85719937]\n",
            "Iterations: 911 / 10000\n",
            "J: 0.6428571540887686\n",
            "Result: y =  0.57 + 0.86 x\n"
          ]
        }
      ]
    }
  ]
}